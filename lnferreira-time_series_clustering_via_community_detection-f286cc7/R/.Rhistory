left_join(news_memberships, twitter_memberships, by = "node") %>% group_by(community.x) %>% count(community.y) %>% slice_head(1)
left_join(news_memberships, twitter_memberships, by = "node") %>% group_by(community.x) %>% count(community.y) %>% slice_head(n = 1)
left_join(news_memberships, twitter_memberships, by = "node") %>% group_by(community.x) %>% count(community.y) %>% slice_max(n)
left_join(news_memberships, twitter_memberships, by = "node") %>% group_by(community.y) %>% count(community.x) %>% slice_max(n)
left_join(news_memberships, twitter_memberships, by = "node") %>% group_by(community.x) %>% count(community.y) %>% slice_max(n)
{
library(tidyverse)
library(lubridate)
library(tidytext)
library(data.table)
library(intergraph)
library(sna)
library(tsna)
library(ndtv)
library(DynComm)
}
load("D:/academicCloud/R/EPINetz/DVPW '21/dvpw_news_edgelist.RDa")
help("DyComm-package")
??("DyComm-package")
??"DyComm-package"
Parameters<-matrix(c("e","0.1","w", "FALSE"),ncol=2, byrow=TRUE)
dc<-DynComm(ALGORITHM$LOUVAIN,CRITERION$MODULARITY,Parameters)
?addRemoveEdges
matrix(
c(10,20,10,30,20,30,30,60,40,60,40,50,50,70,60,70)
,ncol=2,byrow=TRUE)
)
dc$addRemoveEdges(
matrix(
c(10,20,10,30,20,30,30,60,40,60,40,50,50,70,60,70)
,ncol=2,byrow=TRUE)
)
dc<-DynComm(ALGORITHM$LOUVAIN,CRITERION$MODULARITY,Parameters)
dc$addRemoveEdges(
matrix(
c(10,20,10,30,20,30,30,60,40,60,40,50,50,70,60,70)
,ncol=2,byrow=TRUE)
)
dc$communityCount()
## Several alias have been defined.
## In this case, communityNodeCount is alias of communityVertexCount
dc$communityNodeCount(10)
dc$communityNeighbours(10)
dc$communityInnerEdgesWeight(10)
dc$communityTotalWeight(10)
dc$communityEdgeWeight(10,40)
dc$community(10) ##this parameter is a vertex not a community. Do not confuse them
dc$vertices(10)
dc$communityMapping(TRUE)
dc$quality()
dc$time()
## lets add post processing :)
dc$postProcess(
list(
list(POSTPROCESSING$DENSOPT)
)
)
1
dc$communityMapping(TRUE)
dc$communityMapping(TRUE)
## get back to main algorithm results to check they haven't changed
dc$select(POSTPROCESSING$NONE)
## the results of the last step of post processing are selected automatically
## densopt post processing algorithm may change the community mapping so...
## check it
dc$communityMapping(TRUE)
## densopt post processing algorithm may change quality so check it
dc$quality()
## time is now the total time of the main algorithm plus the time of every...
## post processing algorithm up to the one selected
dc$time()
## get back to main algorithm results to check they haven't changed
dc$select(POSTPROCESSING$NONE)
dc$communityMapping(TRUE)
dc$quality()
dc$time()
## add and remove edges. Notice that there is one more column to give...
## weights of zero on the edges to remove. In this case, all other weights...
## are ignored because the graph is set to ignore weights (parameter w is...
## false).
dc$addRemoveEdges(
matrix(
c(30,60,0,40,60,0.23,10,80,2342,80,90,3.1415)
,ncol=3,byrow=TRUE)
)
## since the post processing was not reset, it will be automatically...
## calculated and results switched to the last step. In this case, to the...
## densopt algorithm
dc$communityMapping(TRUE)
dc$quality()
dc$time()
## get back to main algorithm results to check them
dc$select(POSTPROCESSING$NONE)
dc$communityMapping(TRUE)
dc$quality()
dc$time()
## lets reset/remove post processing
dc$postProcess()
load("D:/academicCloud/EPINETZ/EPINetz-Team/Data/DVPW '21/dvpw_timelines.RDa")
colnames(all_timelines)
{
library(devtools)
library(tidyverse)
library(data.table)
}
load_all("D:/academicCloud/R/academictwitteR/")
?bind_tweets
setwd("D:/academicCloud/R/EPINetz/DVPW '21/")
timelines_1 <- bind_tweets(data_path = "data/Timelines 1", output_format = "tidy2", quoted_variables = T)
# timelines 2 (dropout May 11th)
timelines_2 <- bind_tweets(data_path = "data/Timelines 2", output_format = "tidy2", quoted_variables = T)
# timelines 3 (dropout May 18th)
timelines_3 <- bind_tweets(data_path = "data/Timelines 3", output_format = "tidy2", quoted_variables = T)
# timelines 4 (joined May 11th)
timelines_4 <- bind_tweets(data_path = "data/Timelines 4", output_format = "tidy2", quoted_variables = T)
# timelines 5 (joined May 18th)
timelines_5 <- bind_tweets(data_path = "data/Timelines 5", output_format = "tidy2", quoted_variables = T)
all_timelines <- rbindlist(list(timelines_1, timelines_2, timelines_3, timelines_4, timelines_5), use.names = T)
duplicated(all_timelines$tweet_id) %>% sum()
saveRDS(all_timelines, file = "dvpw_timelines_tidy2.RDs")
colnames(all_timelines)
all_timelines %>% filter(is_quote == T) %>% View()
raw <- bind_tweets("data/Timelines 1/", output_format = "raw")
ref <- raw$tweet.referenced_tweets
View(ref)
source_main <- dplyr::select(raw$sourcetweet.main, .data$id, .data$text, .data$lang, .data$author_id) %>%
dplyr::distinct(.data$id, .keep_all = TRUE)
View(source_main)
source <- raw$sourcetweet.main
View(source)
rt <-        # RT and quote entity information. This is incomplete in the tweet.entities for RTs due to truncation and missing for quotes
raw$sourcetweet.main %>%
dplyr::filter(.data$id %in% raw$tweet.referenced_tweets[raw$tweet.referenced_tweets$type != "replied_to", "id"]$id) %>% # get retweets & quotes only
dplyr::distinct(.data$id, .keep_all = TRUE)
View(rt)
rt$entities.hashtags <- rt$entities$hashtags %>% purrr::map(as.data.frame)
hashtags_rt <-
rt %>% dplyr::select(.data$id, .data$entities.hashtags) %>% tidyr::unnest(.data$entities.hashtags, names_sep = "_", keep_empty = T) %>%
dplyr::group_by(.data$id) %>%
dplyr::mutate(hashtags_source = if ("entities.hashtags_tag" %in% colnames(.)) list(.data$entities.hashtags_tag) else NA) %>% # hashtags as list per tweet
dplyr::select(.data$id, .data$hashtags_source) %>% # dplyr::select relevant variables
dplyr::ungroup() %>% dplyr::distinct(.data$id, .keep_all = TRUE) # drop duplicates introduced by tidyr::unnesting
View(hashtags_rt)
View(source)
quote_ids <- all_timelines %>% filter(is_quote == T) %>% select(tweet_id)
hashtags_rt %>% filter(id %in% quote_ids$id) %>% nrow()
raw$tweet.referenced_tweets
raw$tweet.referenced_tweets$id
hashtags_rt %>% filter(id %in% raw$tweet.referenced_tweets$id) %>% nrow()
quote_ids
quote_ids$tweet_id %in% raw$tweet.referenced_tweets$id
quote_ids$tweet_id %in% raw$tweet.referenced_tweets$id %>% sum()
?document
document("D:/academicCloud/R/academictwitteR/")
?bind_tweets
document("D:/academicCloud/R/academictwitteR/")
{
library(tidyverse)
library(BreakoutDetection)
library(scales)
library(zoo)
library(igraph)
}
load("D:/academicCloud/R/EPINetz/DVPW '21/dvpw_twitter_snapshot_measures.RDa")
dynamic_measure_twitter <- dynamic_measure
load("D:/academicCloud/R/EPINetz/DVPW '21/dvpw_news_snapshot_measures.RDa")
dynamic_measure_news <- dynamic_measure
install.packages(fitdistrplus)
install.packages("fitdistrplus")
rm(dynamic_measure)
descdist(dynamic_measure_twitter, discrete = FALSE)
library(fitdistrplus)
descdist(dynamic_measure_twitter, discrete = FALSE)
descdist(dynamic_measure_twitter$degree_tf, discrete = FALSE)
descdist(dynamic_measure_news$degree_tf, discrete = FALSE)
fitdist(dynamic_measure_twitter$degree_tf, "norm") %>% plot()
fitdist(dynamic_measure_twitter$degree_tf, "norm")
fitdist(dynamic_measure_twitter$degree_tf, "gamma")
fitdist(dynamic_measure_twitter$degree_tf, "lognormal")
fitdist(dynamic_measure_twitter$degree_tf, "Weibull")
?fitdist
?descdist
descdist(dynamic_measure_twitter$degree_tf, discrete = FALSE, boot = 1000)
descdist(dynamic_measure_news$degree_tf, discrete = FALSE, boot = 1000)
fitdist(dynamic_measure_twitter$degree_tf, "Weibull")
fitdist(dynamic_measure_twitter$degree_tf, "gamma")
fitdist(dynamic_measure_twitter$degree_tf, "weibull")
is.na(dynamic_measure_news$degree_tf) %>% sum()
?rescale()
fitdist(rescale(dynamic_measure_twitter$degree_tf), "weibull")
?qqplot()
qqplot(dynamic_measure_twitter$degree_tf, distribution = "weibull")
library(car)
qqPlot(dynamic_measure_twitter$degree_tf, distribution = "weibull")
qqPlot(dynamic_measure_twitter$degree_tf, distribution = "weibull",shape=3.783365e-01, scale=5.273310e+03)
?qqPlot
qqPlot(dynamic_measure_twitter$degree_tf, distribution = "weibull")
qqPlot(dynamic_measure_twitter$degree_tf, distribution = "weibull", shape = 1)
library(cluster)
?diana
diana_twitter <- diana(dynamic_measure_twitter$degree_tf, diss = F)
?hclust
?dist
clust_twitter <- hclust(dist(dynamic_measure_twitter$degree_tf))
{
library(tidyverse)
library(BreakoutDetection)
library(scales)
library(zoo)
library(igraph)
library(fitdistrplus)
library(cluster)
library(lubridate)
library(factoextra)
}
install.packages("tsfetures")
install.packages("tsfeatures")
library(tsfeatures)
?tsfeatures
load("D:/academicCloud/R/EPINetz/DVPW '21/dvpw_twitter_snapshot_measures.RDa")
dynamic_measure_twitter <- dynamic_measure
load("D:/academicCloud/R/EPINetz/DVPW '21/dvpw_news_snapshot_measures.RDa")
dynamic_measure_news <- dynamic_measure
load("D:/academicCloud/R/EPINetz/DVPW '21/dvpw_twitter_snapshot_measures.RDa")
dynamic_measure_twitter <- dynamic_measure
load("D:/academicCloud/R/EPINetz/DVPW '21/dvpw_news_snapshot_measures.RDa")
dynamic_measure_news <- dynamic_measure
setwd("D:/academicCloud/EPINETZ/EPINetz-Team/Data/DVPW '21/")
rm(dynamic_measure)
ind_news_features <- tsfeatures(dynamic_measure_news)
View(dynamic_measure_news)
ind_news_features <- dynamic_measure_news %>% filter(node == "baerbock") %>% select(degree_tf) %>% tsfeatures()
dynamic_measure_news %>% filter(node == "baerbock")
dynamic_measure_news %>% filter(node == "baerbock") %>% select(degree_tf)
ind_news_features <- dynamic_measure_news %>% filter(node == "baerbock") %>% dplyr::select(degree_tf) %>% tsfeatures()
View(ind_news_features)
dynamic_measure_news %>% filter(node == "baerbock") %>% dplyr::select(degree_tf) %>% tsfeatures()
dynamic_measure_news %>% filter(node == "abaerbock") %>% dplyr::select(degree_tf) %>% tsfeatures()
dynamic_measure_twitter%>% filter(node == "baerbock") %>% dplyr::select(degree_tf) %>% tsfeatures()
dynamic_measure_twitter%>% filter(node == "abaerbock") %>% dplyr::select(degree_tf) %>% tsfeatures()
dynamic_measure_news %>% filter(node == "baerbock") %>% dplyr::select(degree_tf) %>% stl_features()
news_termdocs <- readRDS("dvpw_news_docs_day.RDs")
twitter_termdocs <- readRDS("dvpw_twitter_docs_day.RDs")
news_termdocs %>% filter(node == "baerbock") %>% dplyr::select(degree_tf) %>% tsfeatures()
news_termdocs %>% filter(token == "baerbock") %>% dplyr::select(doc_count) %>% tsfeatures()
twitter_termdocs %>% filter(token == "baerbock") %>% dplyr::select(doc_count) %>% tsfeatures()
twitter_termdocs %>% filter(token == "baerbock") %>% dplyr::select(doc_count)
news_termdocs %>% filter(token == "baerbock") %>% dplyr::select(doc_count) %>% tsfeatures(features = "stl_features")
dynamic_measure_news %>% filter(node == "baerbock") %>% dplyr::select(degree_tf) %>% tsfeatures(features = "stl_features")
dynamic_measure_twitter %>% filter(node == "baerbock") %>% dplyr::select(degree_tf) %>% tsfeatures(features = "stl_features")
news_termdocs %>% filter(token == "baerbock") %>% dplyr::select(doc_count) %>% tsfeatures(features = "stl_features")
twitter_termdocs %>% filter(token == "baerbock") %>% dplyr::select(doc_count) %>% tsfeatures(features = "stl_features")
for (token in unique(news_termdocs$token)) {
print(token)
}
news_termdocs %>% filter(token == "baerbock") %>% dplyr::select(doc_count) %>% tsfeatures(features = "stl_features")
for (token in unique(news_termdocs$token)) {
stl_features <- news_termdocs %>% filter(token == token) %>% dplyr::select(doc_count) %>% tsfeatures(features = "stl_features")
if (stl_features$seasonal_period != 1) {
print(token)
print(stl_features)
}
}
?apply
?lapply
unique(news_termdocs$token) %>% length()
for (token in unique(head(news_termdocs$token))) {
stl_features <- news_termdocs %>% filter(token == token) %>% dplyr::select(doc_count) %>% tsfeatures(features = "stl_features")
if (stl_features$seasonal_period != 1) {
print(token)
print(stl_features)
}
}
stl_tokens_news <- unique(news_termdocs$token)
stl_tokens_news <- tibble(token = unique(news_termdocs$token))
?ifelse
stl_tokens_news <- tibble(token = unique(news_termdocs$token))
for (token in head(stl_tokens_news$token)) {
stl_features <- news_termdocs %>% filter(token == token) %>% dplyr::select(doc_count) %>% tsfeatures(features = "stl_features")
stl_tokens_news <- stl_tokens_news %>% mutate(seasonal_period, ifelse(token == token, stl_features$seasonal_period, NA))
}
for (token in head(stl_tokens_news$token)) {
stl_features <- news_termdocs %>% filter(token == token) %>% dplyr::select(doc_count) %>% tsfeatures(features = "stl_features")
stl_tokens_news <- stl_tokens_news %>% mutate(seasonal_period = ifelse(token == token, stl_features$seasonal_period, NA))
}
View(stl_tokens_news)
View(stl_features)
for (token in head(stl_tokens_news$token)) {
stl_features <- news_termdocs %>% filter(token == token) %>% dplyr::select(doc_count) %>% tsfeatures(features = "stl_features")
stl_tokens_news <- stl_tokens_news %>% mutate(seasonal_period = case_when(token == token ~ stl_features$seasonal_period,
TRUE ~NA))
}
for (token in head(stl_tokens_news$token)) {
stl_features <- news_termdocs %>% filter(token == token) %>% dplyr::select(doc_count) %>% tsfeatures(features = "stl_features")
stl_tokens_news <- stl_tokens_news %>% mutate(seasonal_period = case_when(token == token ~ stl_features$seasonal_period,
TRUE ~ NA))
}
?where
for (token in head(stl_tokens_news$token)) {
stl_features <- news_termdocs %>% filter(token == token) %>% dplyr::select(doc_count) %>% tsfeatures(features = "stl_features")
stl_tokens_news <- stl_tokens_news %>% mutate(seasonal_period = case_when(where(token == token) ~ stl_features$seasonal_period,
TRUE ~ NA))
}
where()
tidyselect::where()
dplyr::where()
dynamic_measure_news %>% dplyr::select(where(is.factor))
stl_tokens_news$token[1]
stl_tokens_news <- tibble(token = unique(news_termdocs$token))
for (i in head(stl_tokens_news$token)) {
stl_features <- news_termdocs %>% filter(token == stl_tokens_news$token[i]) %>% dplyr::select(doc_count) %>% tsfeatures(features = "stl_features")
stl_tokens_news$seasonal_period[i] <- stl_features$seasonal_period
}
news_termdocs %>% filter(token == stl_tokens_news$token[i])
for (i in 1:head(stl_tokens_news$token)) {
stl_features <- news_termdocs %>% filter(token == stl_tokens_news$token[i]) %>% dplyr::select(doc_count) %>% tsfeatures(features = "stl_features")
stl_tokens_news$seasonal_period[i] <- stl_features$seasonal_period
}
news_termdocs %>% filter(token == stl_tokens_news$token[i])
View(stl_tokens_news)
stl_tokens_news <- tibble(token = unique(news_termdocs$token))
for (i in 1:head(stl_tokens_news$token)) {
stl_features <- news_termdocs %>% filter(token == stl_tokens_news$token[i]) %>% dplyr::select(doc_count) %>% tsfeatures(features = "stl_features")
stl_tokens_news$seasonal_period[i] <- stl_features$seasonal_period
stl_tokens_news$trend[i] <- stl_features$trend
}
stl_tokens_news <- tibble(token = unique(news_termdocs$token)) %>% head()
for (i in 1:stl_tokens_news$token) {
stl_features <- news_termdocs %>% filter(token == stl_tokens_news$token[i]) %>% dplyr::select(doc_count) %>% tsfeatures(features = "stl_features")
stl_tokens_news$seasonal_period[i] <- stl_features$seasonal_period
stl_tokens_news$trend[i] <- stl_features$trend
}
news_termdocs %>% filter(token == stl_tokens_news$token[i])
stl_tokens_news <- tibble(token = unique(news_termdocs$token)) %>% head()
for (i in 1:nrow(stl_tokens_news$token)) {
stl_features <- news_termdocs %>% filter(token == stl_tokens_news$token[i]) %>% dplyr::select(doc_count) %>% tsfeatures(features = "stl_features")
stl_tokens_news$seasonal_period[i] <- stl_features$seasonal_period
stl_tokens_news$trend[i] <- stl_features$trend
}
for (i in 1:nrow(stl_tokens_news)) {
stl_features <- news_termdocs %>% filter(token == stl_tokens_news$token[i]) %>% dplyr::select(doc_count) %>% tsfeatures(features = "stl_features")
stl_tokens_news$seasonal_period[i] <- stl_features$seasonal_period
stl_tokens_news$trend[i] <- stl_features$trend
}
View(stl_tokens_news)
stl_tokens_news <- tibble(token = unique(news_termdocs$token))
for (i in 1:nrow(stl_tokens_news)) {
stl_features <- news_termdocs %>% filter(token == stl_tokens_news$token[i]) %>% dplyr::select(doc_count) %>% tsfeatures(features = "stl_features")
stl_tokens_news$seasonal_period[i] <- stl_features$seasonal_period
stl_tokens_news$trend[i] <- stl_features$trend
}
View(stl_tokens_news)
range(stl_tokens_news)
range(stl_tokens_news$seasonal_period)
stl_tokens_twitter <- tibble(token = unique(twitter_termdocs$token))
for (i in 1:nrow(stl_tokens_twitter)) {
stl_features <- twitter_termdocs %>% filter(token == stl_tokens_twitter$token[i]) %>% dplyr::select(doc_count) %>% tsfeatures(features = "stl_features")
stl_tokens_twitter$seasonal_period[i] <- stl_features$seasonal_period
stl_tokens_twitter$trend[i] <- stl_features$trend
}
range(stl_tokens_twitter$seasonal_period) # no seasonality
library(devtools)
?install()
?github_release()
install_github("https://github.com/lnferreira/time_series_clustering_via_community_detection")
load_all("D:/academicCloud/R/Tools & Scripts/lnferreira-time_series_clustering_via_community_detection-f286cc7/")
source("D:/academicCloud/R/Tools & Scripts/lnferreira-time_series_clustering_via_community_detection-f286cc7/R/")
source("D:/academicCloud/R/Tools & Scripts/lnferreira-time_series_clustering_via_community_detection-f286cc7/R/tsClustAlgs.R")
source("D:/academicCloud/R/Tools & Scripts/lnferreira-time_series_clustering_via_community_detection-f286cc7/R/tsClustAlgs.r")
?source
source(file = "D:/academicCloud/R/Tools & Scripts/lnferreira-time_series_clustering_via_community_detection-f286cc7/R/tsClustAlgs.R")
source(file = "D:/academicCloud/R/Tools & Scripts/lnferreira-time_series_clustering_via_community_detection-f286cc7/R/tsClustAlgs.R")
setwd("D:/academicCloud/R/Tools & Scripts/lnferreira-time_series_clustering_via_community_detection-f286cc7/R/")
source("tsDist.R")
source("tsClustAlgs.R")
install.packages("entropy")
source(file = "D:/academicCloud/R/Tools & Scripts/lnferreira-time_series_clustering_via_community_detection-f286cc7/R/tsClustAlgs.R")
install.packages(DescTools)
install.packages("DescTools")
source(file = "D:/academicCloud/R/Tools & Scripts/lnferreira-time_series_clustering_via_community_detection-f286cc7/R/tsClustAlgs.R")
setwd("D:/academicCloud/EPINETZ/EPINetz-Team/Data/DVPW '21/")
source(file = "D:/academicCloud/R/Tools & Scripts/lnferreira-time_series_clustering_via_community_detection-f286cc7/R/tsClustAlgs.R")
hashtags <- twitter_data %>% as_tibble() %>% dplyr::select(tweet_id, created_at, hashtags) %>%
mutate(date = as_date(created_at)) %>% unnest(hashtags) %>% filter(!is.na(hashtags)) %>% mutate(hashtags = tolower(hashtags)) %>%
group_by(hashtags, date) %>% summarise(n = n())
twitter_data <- readRDS("D:/academicCloud/R/EPINetz/DVPW '21/dvpw_timelines_tidy2.RDs")
hashtags <- twitter_data %>% as_tibble() %>% dplyr::select(tweet_id, created_at, hashtags) %>%
mutate(date = as_date(created_at)) %>% unnest(hashtags) %>% filter(!is.na(hashtags)) %>% mutate(hashtags = tolower(hashtags)) %>%
group_by(hashtags, date) %>% summarise(n = n())
hashtags_total <- hashtags %>% group_by(hashtags) %>% summarise(n = n())
setwd("D:/academicCloud/R/Tools & Scripts/lnferreira-time_series_clustering_via_community_detection-f286cc7/R/")
# read dataset
ts_dataset = read.csv("../dataset/dataset.csv")
View(ts_dataset)
# get time series labels
labels = ts_dataset$class
# remove time series labels
ts_dataset$class = NULL
# transform it into a list
ts_list = lapply(1:nrow(ts_dataset), function(i) ts_dataset[i,])
View(ts_list)
# for every plot, the user should press 'enter' to see the next plot
par(ask=T)
# plot the dataset
matplot(t(ts_dataset), pch=1, lty=1, t="l", xlab="time", ylab="value", main="Time series dataset", col=c(rep(2,10),rep(4,10)))
# calculate the distances for every pair of time series using DTW.
# You can choose anyone distance function in tsDist.R or write your own one.
ts_dist = dist.parallel(tsList = ts_list, distFunc = tsdiss.dtw, cores = 1)
View(ts_dataset)
# Dist matrix normalization
ts_dist = dist.normalize(ts_dist)
# plot the distance matrix
heatmap(ts_dist, main = "Distance matrix")
# create the network connecting the k = 5 nearest neighbors (shortest distances)
net = net.knn.create(dist = ts_dist, k = 5)
# get the net layout, just to plot the network nodes in the same position
net_layout = layout_components(net)
# plot the network
plot(net, layout=net_layout)
# community detection using
communities = cluster_louvain(net)
# plot communities
plot(communities, net, layout=net_layout)
# This is the main function:
# The output is similar to the kmeans function from the stats package.
# Given a distance matrix, it constructs the network, applies a community detection
# algorithm, and returns the membership of each node.
clustering = ts.community.detection(dist = ts_dist, kOrEps = 5, method = "knn")
print(clustering)
hashtags %>% filter(hashtags %in% filter(hashtags_total, n > 5)$hashtags) %>% head()
hashtags %>% # filter(hashtags %in% filter(hashtags_total, n > 5)$hashtags) %>%  # filter for hashtags that appear at least 5 times in total
head()
hashtags %>% # filter(hashtags %in% filter(hashtags_total, n > 5)$hashtags) %>%  # filter for hashtags that appear at least 5 times in total
tail()
hashtags %>% # filter(hashtags %in% filter(hashtags_total, n > 5)$hashtags) %>%  # filter for hashtags that appear at least 5 times in total
tail() %>%
arrange(date) %>% pivot_wider(names_from = date, values_from = n)
?column_to_rownames
hashtags %>% # filter(hashtags %in% filter(hashtags_total, n > 5)$hashtags) %>%  # filter for hashtags that appear at least 5 times in total
tail() %>%
arrange(date) %>% pivot_wider(names_from = date, values_from = n) %>% column_to_rownames(var = "hashtags")
hashtags_wide <- hashtags %>% # filter(hashtags %in% filter(hashtags_total, n > 5)$hashtags) %>%  # filter for hashtags that appear at least 5 times in total
tail() %>%
arrange(date) %>% pivot_wider(names_from = date, values_from = n) %>% column_to_rownames(var = "hashtags")
hashtag_list = lapply(1:nrow(hashtags_wide), function(i) hashtags_wide[i,])
View(hashtag_list)
?rownames(hashtags_wider)
rownames(hashtags_wider)
rownames(hashtags_wide)
hashtags %>% # filter(hashtags %in% filter(hashtags_total, n > 5)$hashtags) %>%  # filter for hashtags that appear at least 5 times in total
tail() %>%
complete(date, hasthags, fill = list(n = 0))
hashtags_wide <- hashtags %>% # filter(hashtags %in% filter(hashtags_total, n > 5)$hashtags) %>%  # filter for hashtags that appear at least 5 times in total
tail() %>%
ungroup() %>% complete(date, hasthags, fill = list(n = 0))
hashtags %>% # filter(hashtags %in% filter(hashtags_total, n > 5)$hashtags) %>%  # filter for hashtags that appear at least 5 times in total
tail() %>%
ungroup() %>% complete(date, hashtags, fill = list(n = 0))
hashtags %>%  # filter(hashtags %in% filter(hashtags_total, n > 5)$hashtags) %>%  # filter for hashtags that appear at least 5 times in total
group_by(hashtags) %>% filter(sum(n) > 5)
hashtags %>%  # filter(hashtags %in% filter(hashtags_total, n > 5)$hashtags) %>%  # filter for hashtags that appear at least 5 times in total
group_by(hashtags) %>% filter(sum(n) > 5) %>% select(hashtags) %>% unique() %>% length()
hashtags %>%  # filter(hashtags %in% filter(hashtags_total, n > 5)$hashtags) %>%  # filter for hashtags that appear at least 5 times in total
group_by(hashtags) %>% filter(sum(n) > 5) %>% dplyr::select(hashtags) %>% unique() %>% length()
hashtags %>%  # filter(hashtags %in% filter(hashtags_total, n > 5)$hashtags) %>%  # filter for hashtags that appear at least 5 times in total
group_by(hashtags) %>% filter(sum(n) > 5) %>% dplyr::select(hashtags) %>% unique()
hashtags_total %>% filter(n > 5) %>% unique()
hashtags_total %>% filter(n > 5) %>% select(hashtags) %>% unique()
hashtags_total %>% filter(n > 5) %>% dplyr::select(hashtags) %>% unique()
hashtags %>%  # filter(hashtags %in% filter(hashtags_total, n > 5)$hashtags) %>%  # filter for hashtags that appear at least 5 times in total
group_by(hashtags) %>% filter(sum(n) > 5)
View(hashtags_total)
View(hashtags)
hashtags_total <- hashtags %>% group_by(hashtags) %>% summarise(n = sum(n))
hashtags_total %>% filter(n > 5) %>% dplyr::select(hashtags) %>% unique()
hashtags_wide <- hashtags %>%
group_by(hashtags) %>% filter(sum(n) > 5) %>%  # filter for hashtags that appear at least 5 times in total
ungroup() %>% complete(date, hashtags, fill = list(n = 0)) %>%
arrange(date) %>% pivot_wider(names_from = date, values_from = n)
hashtags_wide
hashtag_labels <-  hashtags_wide$hashtags
hashtags_wide$hashtags <- NULL
View(hashtags_wide)
hashtag_list = lapply(1:nrow(hashtags_wide), function(i) hashtags_wide[i,])
View(hashtags_total)
View(hashtag_list)
hashtags_wide <- hashtags %>%
group_by(hashtags) %>% filter(sum(n) > 20) %>%  # filter for hashtags that appear at least 5 times in total
ungroup() %>% complete(date, hashtags, fill = list(n = 0)) %>%
arrange(date) %>% pivot_wider(names_from = date, values_from = n)
hashtags_wide <- hashtags %>%
group_by(hashtags) %>% filter(sum(n) > 20) %>%  # filter for hashtags that appear at least 20 times in total
ungroup() %>% complete(date, hashtags, fill = list(n = 0)) %>%
arrange(date) %>% pivot_wider(names_from = date, values_from = n)
hashtag_labels <-  hashtags_wide$hashtags
hashtags_wide$hashtags <- NULL
hashtag_list = lapply(1:nrow(hashtags_wide), function(i) hashtags_wide[i,])
# plot the dataset
matplot(t(ts_dataset), pch=1, lty=1, t="l", xlab="time", ylab="value", main="Time series dataset", col=c(rep(2,10),rep(4,10)))
hashtag_labels <-  hashtags_wide$hashtags
hashtags_wide$hashtags <- NULL
hashtags_wide <- hashtags %>%
group_by(hashtags) %>% filter(sum(n) > 20) %>%  # filter for hashtags that appear at least 20 times in total
ungroup() %>% complete(date, hashtags, fill = list(n = 0)) %>%
arrange(date) %>% pivot_wider(names_from = date, values_from = n)
hashtag_labels <-  hashtags_wide$hashtags
hashtags_wide$hashtags <- NULL
hashtag_list = lapply(1:nrow(hashtags_wide), function(i) hashtags_wide[i,])
matplot(t(hashtags_wide), pch=1, lty=1, t="l", xlab="time", ylab="value", main="Time series dataset", col=c(rep(2,10),rep(4,10)))
hashtag_dist = dist.parallel(tsList = hashtag_list, distFunc = tsdiss.dtw, cores = 5)
hashtag_dist_dtw = dist.parallel(tsList = hashtag_list, distFunc = tsdiss.dtw, cores = 1)
save(hashtag_dist_dtw, file = "D:/academicCloud/R/EPINetz/DVPW '21/hashtag_dist_dtw.Rda")
